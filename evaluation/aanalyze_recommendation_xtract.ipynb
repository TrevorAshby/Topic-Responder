{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.2+cu121 with CUDA 1201 (you have 2.2.2+cu121)\n",
      "    Python  3.9.18 (you have 3.9.16)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "#import gensim\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_instruction_format_recommend(sample, tokenizer):\n",
    "    num_sugg = 3 #len(sample['topics'].split(','))\n",
    "    messages = [\n",
    "        {\n",
    "        \"role\":\"system\",\n",
    "        \"content\": f\"Your goal is to recommend new topics of conversation based on a user\\'s preferences towards topics.\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Generate only {num_sugg} similar topics that could be suggested for new conversation that takes influence from but are not present in the following user profile: {sample['profile']} In the generated answer, generate each of the suggested topics separated by a comma like so: TOPIC1,TOPIC2,TOPIC3,TOPIC4,etc.\"\n",
    "        },\n",
    "        # {\n",
    "        # \"role\": \"assistant\",\n",
    "        # \"content\": f\"{sample['topics']}\"\n",
    "        # }\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "def prompt_instruction_format_xtract(sample, tokenizer):\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "        \"role\":\"system\",\n",
    "        \"content\": f\"Your goal is to extract topics and the speaker\\'s positive preference (yes, unknown, or no) towards the topic from a conversation turn.\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Generate a list of topics increasing in specificity to define the subject of conversation from this utterance: {sample['utterance']}\"\n",
    "        },\n",
    "        # {\n",
    "        # \"role\": \"assistant\",\n",
    "        # \"content\": f\"{sample['topics']}\"\n",
    "        # }\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>utterance</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4766</td>\n",
       "      <td>Not really, I just listen to the radio when I'...</td>\n",
       "      <td>(music,no)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5540</td>\n",
       "      <td>Not really, I'm not that into sports. What abo...</td>\n",
       "      <td>(sports,unknown)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10715</td>\n",
       "      <td>No, I haven't. What kind of food do they serve?</td>\n",
       "      <td>(food,yes)|(restaurant,yes)|(10th street resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2241</td>\n",
       "      <td>I have heard of that one, but I haven't read i...</td>\n",
       "      <td>(hobby,yes), yes)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12650</td>\n",
       "      <td>Not really. I'm not very good at it.</td>\n",
       "      <td>(food,no)|(cooking,no)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          utterance  \\\n",
       "0        4766  Not really, I just listen to the radio when I'...   \n",
       "1        5540  Not really, I'm not that into sports. What abo...   \n",
       "2       10715    No, I haven't. What kind of food do they serve?   \n",
       "3        2241  I have heard of that one, but I haven't read i...   \n",
       "4       12650               Not really. I'm not very good at it.   \n",
       "\n",
       "                                              topics  \n",
       "0                                         (music,no)  \n",
       "1                                   (sports,unknown)  \n",
       "2  (food,yes)|(restaurant,yes)|(10th street resta...  \n",
       "3                                  (hobby,yes), yes)  \n",
       "4                             (food,no)|(cooking,no)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomm_test = pd.read_csv('../CoT/recommender/recommender_data/recommend_test.csv', sep='\\t')\n",
    "xtract_test = pd.read_csv('../CoT/topic_extraction/topic_xtract_data/xtract_test.csv', sep='\\t')\n",
    "xtract_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>profile</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>{\"travel\":\"positive\", \"road trips\":\"positive\",...</td>\n",
       "      <td>road trip destinations,travel safety,road trip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194</td>\n",
       "      <td>{\"gardening\":\"positive\", \"vertical gardening\":...</td>\n",
       "      <td>vertical gardens,soil testing,urban gardening,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>{\"technology\":\"positive\", \"gadgets\":\"positive\"...</td>\n",
       "      <td>tech innovations,app development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>154</td>\n",
       "      <td>{\"art\":\"positive\", \"abstract art\":\"positive\", ...</td>\n",
       "      <td>contemporary art,art criticism,art accessibili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>187</td>\n",
       "      <td>{\"sports\":\"positive\", \"basketball\":\"positive\",...</td>\n",
       "      <td>NBA,sports betting,basketball techniques,baske...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>153</td>\n",
       "      <td>{\"history\":\"positive\", \"Renaissance art\":\"posi...</td>\n",
       "      <td>art history,heist stories,art recovery,famous ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>363</td>\n",
       "      <td>{\"horror movies\":\"negative\", \"video games\":\"ne...</td>\n",
       "      <td>nutrition tips,healthy recipes,fitness challenges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>185</td>\n",
       "      <td>{\"music\":\"positive\", \"folk music\":\"positive\", ...</td>\n",
       "      <td>folklore,cultural issues,folk festivals,folk i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>164</td>\n",
       "      <td>{\"technology\":\"positive\", \"blockchain technolo...</td>\n",
       "      <td>blockchain applications,online security,digita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>78</td>\n",
       "      <td>{\"fitness\":\"positive\", \"swimming\":\"positive\", ...</td>\n",
       "      <td>swimming techniques,pool hygiene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>47</td>\n",
       "      <td>{\"music\":\"positive\", \"rock music\":\"positive\", ...</td>\n",
       "      <td>rock legends,copyright issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>196</td>\n",
       "      <td>{\"science\":\"positive\", \"paleontology\":\"positiv...</td>\n",
       "      <td>dinosaur discoveries,museum heists,prehistoric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>{\"fitness\":\"positive\", \"yoga\":\"positive\"}</td>\n",
       "      <td>healthy eating,meditation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>197</td>\n",
       "      <td>{\"cooking\":\"positive\", \"French cuisine\":\"posit...</td>\n",
       "      <td>French recipes,sustainability,wine pairing,cul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>316</td>\n",
       "      <td>{\"horror movies\":\"negative\", \"art exhibitions\"...</td>\n",
       "      <td>art appreciation,creative expression,art galle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>83</td>\n",
       "      <td>{\"science\":\"positive\", \"genetic engineering\":\"...</td>\n",
       "      <td>gene editing,ethics discussions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>271</td>\n",
       "      <td>{\"horror movies\":\"negative\", \"art exhibitions\"...</td>\n",
       "      <td>art appreciation,creative expression,art galle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>112</td>\n",
       "      <td>{\"travel\":\"positive\", \"volunteer abroad\":\"posi...</td>\n",
       "      <td>volunteer programs,cross-cultural experiences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44</td>\n",
       "      <td>{\"pets\":\"positive\", \"birds\":\"positive\", \"exoti...</td>\n",
       "      <td>birdwatching,pet ownership</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>51</td>\n",
       "      <td>{\"technology\":\"positive\", \"virtual reality\":\"p...</td>\n",
       "      <td>VR experiences,digital detox,tech impact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>176</td>\n",
       "      <td>{\"movies\":\"positive\", \"sci-fi films\":\"positive...</td>\n",
       "      <td>science fiction,diversity in cinema,sci-fi lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>{\"gardening\":\"positive\", \"succulents\":\"positive\"}</td>\n",
       "      <td>indoor plants,gardening tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>339</td>\n",
       "      <td>{\"basketball\":\"negative\", \"yoga\":\"positive\", \"...</td>\n",
       "      <td>yoga practices,yoga styles,meditative yoga,min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>54</td>\n",
       "      <td>{\"art\":\"positive\", \"modern art\":\"positive\", \"a...</td>\n",
       "      <td>contemporary artists,art investments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>376</td>\n",
       "      <td>{\"romantic comedies\":\"negative\", \"pop music\":\"...</td>\n",
       "      <td>hiking trails,camping tips,national parks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>98</td>\n",
       "      <td>{\"music\":\"positive\", \"pop music\":\"positive\", \"...</td>\n",
       "      <td>music charts,artist compensation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>52</td>\n",
       "      <td>{\"fitness\":\"positive\", \"cycling\":\"positive\", \"...</td>\n",
       "      <td>bike trails,recovery strategies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18</td>\n",
       "      <td>{\"history\":\"positive\", \"World War II\":\"positiv...</td>\n",
       "      <td>war documentaries,history lessons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>362</td>\n",
       "      <td>{\"reality TV\":\"negative\", \"celebrity gossip\":\"...</td>\n",
       "      <td>book clubs,book reviews,book adaptations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13</td>\n",
       "      <td>{\"sports\":\"positive\", \"basketball\":\"positive\",...</td>\n",
       "      <td>basketball players,sports news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>299</td>\n",
       "      <td>{\"romantic comedies\":\"negative\", \"outdoor phot...</td>\n",
       "      <td>photography techniques,camera gear,nature expl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>145</td>\n",
       "      <td>{\"pets\":\"positive\", \"cats\":\"positive\", \"declaw...</td>\n",
       "      <td>cat care,animal welfare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>251</td>\n",
       "      <td>{\"fast food\":\"negative\", \"cooking at home\":\"po...</td>\n",
       "      <td>home cooking,healthy recipes,chef techniques,c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>327</td>\n",
       "      <td>{\"fast fashion\":\"negative\", \"vintage fashion\":...</td>\n",
       "      <td>vintage finds,thrifting tips,second-hand fashi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>{\"music\":\"positive\", \"classical music\":\"positi...</td>\n",
       "      <td>music history,music genres,studio production</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>57</td>\n",
       "      <td>{\"science\":\"positive\", \"neuroscience\":\"positiv...</td>\n",
       "      <td>brain studies,neuroethics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>178</td>\n",
       "      <td>{\"fitness\":\"positive\", \"crossfit\":\"positive\", ...</td>\n",
       "      <td>crossfit workouts,recovery methods,crossfit co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>247</td>\n",
       "      <td>{\"travel\":\"positive\", \"road trips\":\"positive\",...</td>\n",
       "      <td>road trip destinations,travel safety,road trip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>95</td>\n",
       "      <td>{\"pets\":\"positive\", \"cats\":\"positive\", \"declaw...</td>\n",
       "      <td>cat care,animal welfare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>162</td>\n",
       "      <td>{\"travel\":\"positive\", \"volunteer abroad\":\"posi...</td>\n",
       "      <td>volunteer programs,cross-cultural experiences,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                            profile  \\\n",
       "0          225  {\"travel\":\"positive\", \"road trips\":\"positive\",...   \n",
       "1          194  {\"gardening\":\"positive\", \"vertical gardening\":...   \n",
       "2            3  {\"technology\":\"positive\", \"gadgets\":\"positive\"...   \n",
       "3          154  {\"art\":\"positive\", \"abstract art\":\"positive\", ...   \n",
       "4          187  {\"sports\":\"positive\", \"basketball\":\"positive\",...   \n",
       "5          153  {\"history\":\"positive\", \"Renaissance art\":\"posi...   \n",
       "6          363  {\"horror movies\":\"negative\", \"video games\":\"ne...   \n",
       "7          185  {\"music\":\"positive\", \"folk music\":\"positive\", ...   \n",
       "8          164  {\"technology\":\"positive\", \"blockchain technolo...   \n",
       "9           78  {\"fitness\":\"positive\", \"swimming\":\"positive\", ...   \n",
       "10          47  {\"music\":\"positive\", \"rock music\":\"positive\", ...   \n",
       "11         196  {\"science\":\"positive\", \"paleontology\":\"positiv...   \n",
       "12           4          {\"fitness\":\"positive\", \"yoga\":\"positive\"}   \n",
       "13         197  {\"cooking\":\"positive\", \"French cuisine\":\"posit...   \n",
       "14         316  {\"horror movies\":\"negative\", \"art exhibitions\"...   \n",
       "15          83  {\"science\":\"positive\", \"genetic engineering\":\"...   \n",
       "16         271  {\"horror movies\":\"negative\", \"art exhibitions\"...   \n",
       "17         112  {\"travel\":\"positive\", \"volunteer abroad\":\"posi...   \n",
       "18          44  {\"pets\":\"positive\", \"birds\":\"positive\", \"exoti...   \n",
       "19          51  {\"technology\":\"positive\", \"virtual reality\":\"p...   \n",
       "20         176  {\"movies\":\"positive\", \"sci-fi films\":\"positive...   \n",
       "21          20  {\"gardening\":\"positive\", \"succulents\":\"positive\"}   \n",
       "22         339  {\"basketball\":\"negative\", \"yoga\":\"positive\", \"...   \n",
       "23          54  {\"art\":\"positive\", \"modern art\":\"positive\", \"a...   \n",
       "24         376  {\"romantic comedies\":\"negative\", \"pop music\":\"...   \n",
       "25          98  {\"music\":\"positive\", \"pop music\":\"positive\", \"...   \n",
       "26          52  {\"fitness\":\"positive\", \"cycling\":\"positive\", \"...   \n",
       "27          18  {\"history\":\"positive\", \"World War II\":\"positiv...   \n",
       "28         362  {\"reality TV\":\"negative\", \"celebrity gossip\":\"...   \n",
       "29          13  {\"sports\":\"positive\", \"basketball\":\"positive\",...   \n",
       "30         299  {\"romantic comedies\":\"negative\", \"outdoor phot...   \n",
       "31         145  {\"pets\":\"positive\", \"cats\":\"positive\", \"declaw...   \n",
       "32         251  {\"fast food\":\"negative\", \"cooking at home\":\"po...   \n",
       "33         327  {\"fast fashion\":\"negative\", \"vintage fashion\":...   \n",
       "34          34  {\"music\":\"positive\", \"classical music\":\"positi...   \n",
       "35          57  {\"science\":\"positive\", \"neuroscience\":\"positiv...   \n",
       "36         178  {\"fitness\":\"positive\", \"crossfit\":\"positive\", ...   \n",
       "37         247  {\"travel\":\"positive\", \"road trips\":\"positive\",...   \n",
       "38          95  {\"pets\":\"positive\", \"cats\":\"positive\", \"declaw...   \n",
       "39         162  {\"travel\":\"positive\", \"volunteer abroad\":\"posi...   \n",
       "\n",
       "                                               topics  \n",
       "0   road trip destinations,travel safety,road trip...  \n",
       "1   vertical gardens,soil testing,urban gardening,...  \n",
       "2                    tech innovations,app development  \n",
       "3   contemporary art,art criticism,art accessibili...  \n",
       "4   NBA,sports betting,basketball techniques,baske...  \n",
       "5   art history,heist stories,art recovery,famous ...  \n",
       "6   nutrition tips,healthy recipes,fitness challenges  \n",
       "7   folklore,cultural issues,folk festivals,folk i...  \n",
       "8   blockchain applications,online security,digita...  \n",
       "9                    swimming techniques,pool hygiene  \n",
       "10                      rock legends,copyright issues  \n",
       "11  dinosaur discoveries,museum heists,prehistoric...  \n",
       "12                          healthy eating,meditation  \n",
       "13  French recipes,sustainability,wine pairing,cul...  \n",
       "14  art appreciation,creative expression,art galle...  \n",
       "15                    gene editing,ethics discussions  \n",
       "16  art appreciation,creative expression,art galle...  \n",
       "17      volunteer programs,cross-cultural experiences  \n",
       "18                         birdwatching,pet ownership  \n",
       "19           VR experiences,digital detox,tech impact  \n",
       "20  science fiction,diversity in cinema,sci-fi lit...  \n",
       "21                      indoor plants,gardening tools  \n",
       "22  yoga practices,yoga styles,meditative yoga,min...  \n",
       "23               contemporary artists,art investments  \n",
       "24          hiking trails,camping tips,national parks  \n",
       "25                   music charts,artist compensation  \n",
       "26                    bike trails,recovery strategies  \n",
       "27                  war documentaries,history lessons  \n",
       "28           book clubs,book reviews,book adaptations  \n",
       "29                     basketball players,sports news  \n",
       "30  photography techniques,camera gear,nature expl...  \n",
       "31                            cat care,animal welfare  \n",
       "32  home cooking,healthy recipes,chef techniques,c...  \n",
       "33  vintage finds,thrifting tips,second-hand fashi...  \n",
       "34       music history,music genres,studio production  \n",
       "35                          brain studies,neuroethics  \n",
       "36  crossfit workouts,recovery methods,crossfit co...  \n",
       "37  road trip destinations,travel safety,road trip...  \n",
       "38                            cat care,animal welfare  \n",
       "39  volunteer programs,cross-cultural experiences,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = api.load('word2vec-google-news-300') #choose from multiple models https://github.com/RaRe-Technologies/gensim-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catch(func, handle=lambda e : e, *args, **kwargs):\n",
    "    try:\n",
    "        return func(*args, **kwargs)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def preprocess(s):\n",
    "    return [i.lower() for i in s.split()]\n",
    "\n",
    "def get_vector(s):\n",
    "    # ignore any words that are not within glove\n",
    "    vecs = [catch(lambda: glove[i]) for i in preprocess(s)]\n",
    "    vecs = [v for v in vecs if type(v) != type(None)]\n",
    "    # print(vecs)\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(300)\n",
    "    return np.sum(np.array(vecs), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval 7b recommend model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "llm_path = '../CoT/recommender/hf_output/checkpoint-220'\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_path)\n",
    "llm_model.to('cuda:0')\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_path)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_tokenizer.padding_side = 'right'\n",
    "llm_pipeline = pipeline('text-generation', model=llm_model, \n",
    "                                tokenizer=llm_tokenizer, torch_dtype=torch.float32, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      " 10%|█         | 4/40 [00:05<00:44,  1.25s/it]/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/scipy-1.12.0-py3.9-linux-x86_64.egg/scipy/spatial/distance.py:647: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dist = 1.0 - uv / math.sqrt(uu * vv)\n",
      "100%|██████████| 40/40 [00:53<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.7241904125213284\n",
      "recall:  0.7516811736508903\n",
      "f1:  0.7376797606661606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "score2 = 0\n",
    "rec_file = open('./aa_7b.tsv', 'w')\n",
    "#rec_file.write(f\"targets\\tresponse\\tscore\\n\")\n",
    "rec_file.write(f\"targets\\tresponse\\tprecision\\trecall\\tf1\\n\")\n",
    "for i in tqdm(range(len(recomm_test))):\n",
    "    inst = recomm_test.iloc[i]\n",
    "    #print(inst)\n",
    "    \n",
    "    prompt = prompt_instruction_format_recommend(inst, llm_tokenizer)\n",
    "    outputs = llm_pipeline(prompt, max_new_tokens=100, do_sample=True, \n",
    "                                temperature=0.85, top_k=50, top_p=0.95)\n",
    "    response = outputs[0]['generated_text'].split('[/INST]')[-1].strip().split(',')\n",
    "    #print('prof: ', inst['profile'])\n",
    "    targets = inst['topics'].split(',')\n",
    "    #print('old: ', targets)\n",
    "    #print()\n",
    "    #print('new: ', response)\n",
    "    \n",
    "    # modify score based upon number of suggestions\n",
    "    #score -= abs(len(response) - 3)\n",
    "\n",
    "    avg_pred_trg = np.average([np.max([1 - spatial.distance.cosine(get_vector(y_hat), get_vector(y)) for y_hat in response]) for y in targets])\n",
    "    avg_pred_trg2 = np.average([np.max([1 - spatial.distance.cosine(get_vector(y_hat), get_vector(y)) for y_hat in targets]) for y in response]) # recall\n",
    "    score += avg_pred_trg\n",
    "    score2 += avg_pred_trg2\n",
    "    rec_file.write(f\"{inst['topics']}\\t{targets}\\t{response}\\t{avg_pred_trg}\\t{avg_pred_trg2}\\t{(2 * avg_pred_trg * avg_pred_trg2)/(avg_pred_trg + avg_pred_trg2)}\\n\")\n",
    "    \n",
    "print('precision: ', score/len(recomm_test))\n",
    "print('recall: ', score2/len(recomm_test))\n",
    "print('f1: ', (2 * (score/len(recomm_test)) * (score2/len(recomm_test)))/(score/len(recomm_test) + score2/len(recomm_test)))\n",
    "rec_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval 1b Recommend model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendation(xtract_prof, tok_in, mod_in):\n",
    "    num_sugg = 3\n",
    "    prompt = f\"Instruction: Generate only {num_sugg} similar topics that could be suggested for new conversation that takes influence from but are not present in the following user profile: {xtract_prof} In the generated answer, generate each of the suggested topics separated by a comma like so: TOPIC1,TOPIC2,TOPIC3,TOPIC4,etc.\\nSuggested Topics:\"           \n",
    "    tok_text = tok_in(prompt, return_tensors='pt').to('cuda:0')\n",
    "    gen_text = mod_in.generate(**tok_text, max_new_tokens=100)#, do_sample=True, \n",
    "                                #temperature=0.85, top_k=50, top_p=0.95)\n",
    "    dec_text = tok_in.decode(gen_text[0], skip_special_tokens=True)\n",
    "    return dec_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_path = '../CoT/recommender/hf_model_1b/'\n",
    "\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(llm_path)\n",
    "llm_model.to('cuda:0')\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_path)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_tokenizer.padding_side = 'right'\n",
    "# llm_pipeline = pipeline('text-generation', model=llm_model, \n",
    "#                                 tokenizer=llm_tokenizer, torch_dtype=torch.float32, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:11<00:00,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.724823691429133\n",
      "recall:  0.7834936854952763\n",
      "f1:  0.7530176261578174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "score2 = 0\n",
    "rec_file = open('./aa_1b.tsv', 'w')\n",
    "#rec_file.write(f\"targets\\tresponse\\tscore\\n\")\n",
    "rec_file.write(f\"targets\\tresponse\\tprecision\\trecall\\tf1\\n\")\n",
    "for i in tqdm(range(len(recomm_test))):\n",
    "    inst = recomm_test.iloc[i]\n",
    "    prof = inst['profile'].split(',')\n",
    "    response = generate_recommendation(prof, llm_tokenizer, llm_model).strip().split(',')\n",
    "    targets = inst['topics'].split(',')\n",
    "    # score -= abs(len(response) - 3)\n",
    "\n",
    "    avg_pred_trg = np.average([np.max([1 - spatial.distance.cosine(get_vector(y_hat), get_vector(y)) for y_hat in response]) for y in targets]) # precision\n",
    "    avg_pred_trg2 = np.average([np.max([1 - spatial.distance.cosine(get_vector(y_hat), get_vector(y)) for y_hat in targets]) for y in response]) # recall\n",
    "    score += avg_pred_trg\n",
    "    score2 += avg_pred_trg2\n",
    "    rec_file.write(f\"{inst['topics']}\\t{targets}\\t{response}\\t{avg_pred_trg}\\t{avg_pred_trg2}\\t{(2 * avg_pred_trg * avg_pred_trg2)/(avg_pred_trg + avg_pred_trg2)}\\n\")\n",
    "    \n",
    "print('precision: ', score/len(recomm_test))\n",
    "print('recall: ', score2/len(recomm_test))\n",
    "print('f1: ', (2 * (score/len(recomm_test)) * (score2/len(recomm_test)))/(score/len(recomm_test) + score2/len(recomm_test)))\n",
    "rec_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval 7b extract model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "llm_path = '../CoT/topic_extraction/hf_output/checkpoint-3534'\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(llm_path)\n",
    "llm_model.to('cuda:0')\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(llm_path)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_tokenizer.padding_side = 'right'\n",
    "llm_pipeline = pipeline('text-generation', model=llm_model, \n",
    "                                tokenizer=llm_tokenizer, torch_dtype=torch.float32, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1335 [00:07<16:24,  1.35it/s]/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "100%|██████████| 1335/1335 [17:06<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.7607818071688531\n",
      "recall:  0.7660989008489488\n",
      "f1:  0.7634310960868332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "score2 = 0\n",
    "rec_file = open('./aa_7b_x_1178.tsv', 'w')\n",
    "rec_file.write(f\"targets\\tresponse\\tprecision\\trecall\\tf1\\n\")\n",
    "for i in tqdm(range(len(xtract_test))):\n",
    "    inst = xtract_test.iloc[i]\n",
    "    #print(inst)\n",
    "    \n",
    "    prompt = prompt_instruction_format_xtract(inst, llm_tokenizer)\n",
    "    outputs = llm_pipeline(prompt, max_new_tokens=100, do_sample=True, \n",
    "                                temperature=0.85, top_k=50, top_p=0.95)\n",
    "    response = outputs[0]['generated_text'].split('[/INST]')[-1].strip().split('|')\n",
    "    response = [r.replace(',', ' ') for r in response]\n",
    "    response = [r.replace('(', ' ') for r in response]\n",
    "    response = [r.replace(')', ' ') for r in response]\n",
    "    response = [r.replace('[', ' ') for r in response]\n",
    "    response = [r.replace(']', ' ') for r in response]\n",
    "    #print(response)\n",
    "    targets = inst['topics'].split('|')\n",
    "    targets = [t.replace(',', ' ') for t in targets]\n",
    "    targets = [t.replace('(', ' ') for t in targets]\n",
    "    targets = [t.replace(')', ' ') for t in targets]\n",
    "    targets = [t.replace('[', ' ') for t in targets]\n",
    "    targets = [t.replace(']', ' ') for t in targets]\n",
    "    \n",
    "    # modify score based upon number of suggestions\n",
    "    # score -= abs(len(response) - 3)\n",
    "\n",
    "    avg_pred_trg = np.average([np.max([1 - spatial.distance.cosine(get_vector(y_hat), get_vector(y)) for y_hat in response]) for y in targets]) # precision\n",
    "    avg_pred_trg2 = np.average([np.max([1 - spatial.distance.cosine(get_vector(y_hat), get_vector(y)) for y_hat in targets]) for y in response]) # recall\n",
    "    score += avg_pred_trg\n",
    "    score2 += avg_pred_trg2\n",
    "    rec_file.write(f\"{inst['topics']}\\t{targets}\\t{response}\\t{avg_pred_trg}\\t{avg_pred_trg2}\\t{(2 * avg_pred_trg * avg_pred_trg2)/(avg_pred_trg + avg_pred_trg2)}\\n\")\n",
    "    \n",
    "print('precision: ', score/len(xtract_test))\n",
    "print('recall: ', score2/len(xtract_test))\n",
    "print('f1: ', (2 * (score/len(xtract_test)) * (score2/len(xtract_test)))/(score/len(xtract_test) + score2/len(xtract_test)))\n",
    "rec_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval 1b extract model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cot(text_in, tok_in, mod_in):\n",
    "    instruction = \"Instruction: Generate a list of topics increasing in specificity to define the subject of conversation.\\n\"\n",
    "    instruction += f\"Input:{text_in}\"\n",
    "    formatted_prompt = (f\"<|im_start|>user\\n{instruction}<|im_end|>\\n<|im_start|>assistant\\nThe topics defining the input are:\")\n",
    "    tok_text = tok_in(formatted_prompt, return_tensors='pt').to('cuda:0')\n",
    "    gen_text = mod_in.generate(**tok_text, max_new_tokens=60)\n",
    "    dec_text = tok_in.decode(gen_text[0], skip_special_tokens=True)\n",
    "    #print(dec_text)\n",
    "    dec_text = re.search('```.*\\n```', dec_text).group()[3:-4]\n",
    "\n",
    "    return dec_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_tokenizer = AutoTokenizer.from_pretrained(\"../CoT/topic_extraction/hf_model_1b/\")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\"../CoT/topic_extraction/hf_model_1b/\")\n",
    "llm_model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "score2 = 0\n",
    "rec_file = open('./aa_1b_x.tsv', 'w')\n",
    "rec_file.write(f\"targets\\tresponse\\tprecision\\trecall\\tf1\\n\")\n",
    "for i in tqdm(range(len(xtract_test))):\n",
    "    inst = xtract_test.iloc[i]\n",
    "    response = generate_cot(inst['utterance'], llm_tokenizer, llm_model).strip().split('|')\n",
    "    \n",
    "    response = [r.replace(',', ' ') for r in response]\n",
    "    response = [r.replace('(', ' ') for r in response]\n",
    "    response = [r.replace(')', ' ') for r in response]\n",
    "    response = [r.replace('[', ' ') for r in response]\n",
    "    response = [r.replace(']', ' ') for r in response]\n",
    "    #print(response)\n",
    "    targets = inst['topics'].split('|')\n",
    "    targets = [t.replace(',', ' ') for t in targets]\n",
    "    targets = [t.replace('(', ' ') for t in targets]\n",
    "    targets = [t.replace(')', ' ') for t in targets]\n",
    "    targets = [t.replace('[', ' ') for t in targets]\n",
    "    targets = [t.replace(']', ' ') for t in targets]\n",
    "    \n",
    "    avg_pred_trg = np.average([np.max([1 - spatial.distance.cosine(get_vector(y_hat), get_vector(y)) for y_hat in response]) for y in targets]) # precision\n",
    "    avg_pred_trg2 = np.average([np.max([1 - spatial.distance.cosine(get_vector(y_hat), get_vector(y)) for y_hat in targets]) for y in response]) # recall\n",
    "    score += avg_pred_trg\n",
    "    score2 += avg_pred_trg2\n",
    "    rec_file.write(f\"{inst['utterance']}\\t{targets}\\t{response}\\t{avg_pred_trg}\\t{avg_pred_trg2}\\t{(2 * avg_pred_trg * avg_pred_trg2)/(avg_pred_trg + avg_pred_trg2)}\\n\")\n",
    "    \n",
    "print('precision: ', score/len(xtract_test))\n",
    "print('recall: ', score2/len(xtract_test))\n",
    "print('f1: ', (2 * (score/len(xtract_test)) * (score2/len(xtract_test)))/(score/len(xtract_test) + score2/len(xtract_test)))\n",
    "rec_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic-responder-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
