{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trevi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 787/787 [00:00<00:00, 784kB/s]\n",
      "c:\\Users\\Trevi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Trevi\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading pytorch_model.bin: 100%|██████████| 3.13G/3.13G [01:28<00:00, 35.5MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 142/142 [00:00<00:00, 142kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 1.15k/1.15k [00:00<?, ?B/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.57k/1.57k [00:00<00:00, 1.57MB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 127k/127k [00:00<00:00, 2.64MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 62.9k/62.9k [00:00<00:00, 281kB/s]\n",
      "Downloading (…)in/added_tokens.json: 100%|██████████| 16.0/16.0 [00:00<00:00, 16.0kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 772/772 [00:00<?, ?B/s] \n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.62k/1.62k [00:00<?, ?B/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.46G/1.46G [00:24<00:00, 60.6MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BlenderbotForConditionalGeneration(\n",
       "  (model): BlenderbotModel(\n",
       "    (shared): Embedding(8008, 1280, padding_idx=0)\n",
       "    (encoder): BlenderbotEncoder(\n",
       "      (embed_tokens): Embedding(8008, 1280, padding_idx=0)\n",
       "      (embed_positions): BlenderbotLearnedPositionalEmbedding(128, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x BlenderbotEncoderLayer(\n",
       "          (self_attn): BlenderbotAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BlenderbotDecoder(\n",
       "      (embed_tokens): Embedding(8008, 1280, padding_idx=0)\n",
       "      (embed_positions): BlenderbotLearnedPositionalEmbedding(128, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlenderbotDecoderLayer(\n",
       "          (self_attn): BlenderbotAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BlenderbotAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=8008, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_id = '0'\n",
    "device = torch.device('cuda:'+device_id)\n",
    "\n",
    "guideliner_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "guideliner = T5ForConditionalGeneration.from_pretrained(\"TrevorAshby/guideliner\")\n",
    "\n",
    "blen_tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "blen_model = AutoModelForSeq2SeqLM.from_pretrained(\"TrevorAshby/blenderbot-400M-distill\")\n",
    "\n",
    "guideliner.to(device)\n",
    "blen_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_response=\"B: Hi! Yes, I am a huge fan of Star Wars! I also loved how it was the first major movie to be dubbed in Navajo! I think that is so cool! A: Yeah, that's wild. I guess it takes a pretty small audience to pay for the translation work.\"\n",
    "topic_pref=\"{\\\"high-level\\\":{\\\"topic\\\":\\\"movie\\\", \\\"if-interest\\\":\\\"yes\\\"}, \\\"low-level\\\":{\\\"topic\\\":\\\"Star Wars\\\", \\\"if-interest\\\":\\\"yes\\\"}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user is a huge fan of Star Wars and have a high interest about it. Talk about their favorite scenes from the movie and suggest similar movies to watch.\n"
     ]
    }
   ],
   "source": [
    "guide_in_str = '{}| {}'.format(user_response, topic_pref)\n",
    "in_ids = guideliner_tokenizer(guide_in_str, max_length=256, padding='max_length', return_tensors='pt').input_ids\n",
    "guideline_example = guideliner.generate(in_ids.to(device), max_new_tokens=50)\n",
    "guideline = guideliner_tokenizer.decode(guideline_example[0], skip_special_tokens=True)\n",
    "print(guideline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " That's really cool! Do you have any other favorite movies or movies from the Star Wars franchise?\n"
     ]
    }
   ],
   "source": [
    "blend_in_str = user_response\n",
    "blend_in_str2 = ' [GUIDELINE] ' + guideline\n",
    "blend_in_ids = blen_tokenizer([user_response + blend_in_str2], max_length=128, return_tensors='pt', truncation=True)\n",
    "blend_example = blen_model.generate(**blend_in_ids.to(device), max_length=60)\n",
    "blend_response = blen_tokenizer.batch_decode(blend_example, skip_special_tokens=True)[0]\n",
    "generated_response = blend_response\n",
    "print(generated_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
