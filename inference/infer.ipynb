{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are a chatbot agent. <</SYS>>\n",
      "Generate the next response turn in this conversation: Yeah, their services are good. I'm just not a fan of intrusive they can be on our personal lives.  Google is leading the alphabet subsidiary and will continue to be the Umbrella company for Alphabet internet interest. Did you know Google had hundreds of live goats to cut the grass in the past? \n",
      " Make sure that the response is 1-2 sentences long and compliant with this guideline: The user likes technology. Direct the conversation to one of the following 3 topics: ['technology', 'science', 'technology trends']. [/INST]  Sure, here's the next response turn:\n",
      "\n",
      "\"Ah, I see. Well, technology has come a long way in recent years, and it's exciting to see how it's changing the way we live and work. Did you know that Google is also investing heavily in artificial intelligence and machine learning? They're using these technologies to develop new products and services that can help us automate tasks and make our lives even more convenient.\"\n"
     ]
    }
   ],
   "source": [
    "# set the device\n",
    "device = torch.device(\"cuda:3\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# quantization_config = GPTQConfig(bits=4, dataset = \"c4\", tokenizer=tokenizer)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, token='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',torch_dtype=torch.float32)#, device_map='auto'), quantization_config=quantization_config)\n",
    "\n",
    "model = torch.nn.DataParallel(model, device_ids=[0,1])\n",
    "model.to(device)\n",
    "user_in = 'Yeah, their services are good. I\\'m just not a fan of intrusive they can be on our personal lives. </s> <s>Google is leading the alphabet subsidiary and will continue to be the Umbrella company for Alphabet internet interest.</s> <s>Did you know Google had hundreds of live goats to cut the grass in the past? \\n'\n",
    "guideline = \"The user likes technology. Direct the conversation to one of the following 3 topics: ['technology', 'science', 'technology trends'].\"\n",
    "instruction = f\"<s>[INST] <<SYS>>\\nYou are a chatbot agent. <</SYS>>\\nGenerate the next response turn in this conversation: {user_in.replace('</s> <s>', ' ')} Make sure that the response is 1-2 sentences long and compliant with this guideline: {guideline} [/INST]\"\n",
    "\n",
    "prompt = f\"{instruction}\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.module.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic-responder-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
