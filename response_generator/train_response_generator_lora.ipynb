{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.2+cu121 with CUDA 1201 (you have 2.2.2+cu121)\n",
      "    Python  3.9.18 (you have 3.9.16)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.42s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./rg_output/checkpoint-24825/\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./rg_output/checkpoint-24825/\",torch_dtype=torch.float32)\n",
    "model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_func(sample, token_in, with_guideline):\n",
    "    if 'agent_1' in sample['response']:\n",
    "        p_in = 'agent_1'\n",
    "        not_p_in = 'agent_2'\n",
    "    else:\n",
    "        p_in = 'agent_2'\n",
    "        not_p_in = 'agent_1'\n",
    "\n",
    "    mod_dial = sample['dialogue'].replace('person 1', 'agent_1')\n",
    "    mod_dial = mod_dial.replace('person 2', 'agent_2')\n",
    "    messages = [\n",
    "        {\n",
    "        \"role\":\"system\",\n",
    "        \"content\": f\"You are participating in the conversation. You are specifically {p_in}.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    if with_guideline:\n",
    "        messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Generate the next conversation turn for {p_in} responding to {not_p_in} in this conversation: {mod_dial} Limit the generated response to 1-2 sentences and compliant with this guideline: {sample['guideline']}\"\n",
    "        })\n",
    "    else:\n",
    "        messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Generate the next conversation turn for {p_in} responding to {not_p_in} in this conversation: {mod_dial} Limit the generated response to 1-2 sentences.\"\n",
    "        })\n",
    "\n",
    "    prompt = token_in.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {'train':'../data/lora_ft_train2_copy.csv', \n",
    "              'test':'../data/lora_ft_test2_copy.csv'}\n",
    "dataset = load_dataset('csv', data_files=data_files, delimiter='\\t', column_names=['dialogue','response','guideline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialogue': 'agent_1:hi, do you like netflix?',\n",
       " 'response': \"agent_2:Hello!  I'm a big fan and have had it for years.  What about you?\",\n",
       " 'guideline': \"agent_1 likes hobby. agent_2's response should fall into one of the following 3 topics: ['hobby', 'TV', 'Netflix'].\",\n",
       " '__index_level_0__': 8610.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello!  I'm a big fan and have had it for years.  What about you? \n"
     ]
    }
   ],
   "source": [
    "user_in = dataset['train'][1]['dialogue']\n",
    "guideline = dataset['train'][1]['guideline']\n",
    "current_person_predict = dataset['train'][1]['response']\n",
    "\n",
    "llama_in = form_func({'dialogue':f'{user_in}', 'response':f'{current_person_predict}', 'guideline':f'{guideline}'}, \n",
    "                                         tokenizer, True)\n",
    "blend_in_ids = tokenizer(llama_in, max_length=1024, return_tensors='pt', truncation=True).to('cuda:0')\n",
    "blend_example = model.generate(blend_in_ids.input_ids, max_new_tokens=100, temperature=0.8, top_k=50, top_p = 0.85)\n",
    "our_response = tokenizer.batch_decode(blend_example, skip_special_tokens=True)[0].split('[/INST]')[-1]\n",
    "our_response = our_response.replace('agent_1:', '')\n",
    "our_response = our_response.replace('agent_2:', '')\n",
    "print(our_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n",
    "# print(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token='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',torch_dtype=torch.float32)\n",
    "model.to('cuda:0')\n",
    "\n",
    "# Makes training faster but a little less accurate\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# setting padding instructions for tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_func(sample):\n",
    "    if 'agent_1' in sample['response']:\n",
    "        p_in = 'agent_1'\n",
    "        not_p_in = 'agent_2'\n",
    "    else:\n",
    "        p_in = 'agent_2'\n",
    "        not_p_in = 'agent_1'\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "        \"role\":\"system\",\n",
    "        \"content\": f\"You are participating in the conversation. You are specifically {p_in}.\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Generate the next conversation turn for {p_in} responding to {not_p_in} in this conversation: {sample['dialogue']} Limit the generated response to 1-2 sentences.\"\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"{p_in}:{sample['response']}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainer\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir=f'rg_output',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-3\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "      lora_alpha=16,\n",
    "      lora_dropout=0.1,\n",
    "      r=64,\n",
    "      bias=\"none\",\n",
    "      task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[f'train'],\n",
    "    eval_dataset = dataset[f'test'],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=form_func,\n",
    "    args=trainingArgs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vicuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"lmsys/vicuna-7b-v1.5\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"lmsys/vicuna-7b-v1.5\",torch_dtype=torch.float32)\n",
    "model.to('cuda:0')\n",
    "\n",
    "# Makes training faster but a little less accurate\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# setting padding instructions for tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = {'train':'../data/lora_ft_train.csv', \n",
    "              'test':'../data/lora_ft_test.csv'}\n",
    "dataset = load_dataset('csv', data_files=data_files, delimiter='\\t', column_names=['dialogue', 'response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_func(sample):\n",
    "    prompt = sample['dialogue'] + '\\n' + sample['response']\n",
    "    prompt = prompt.replace('agent_1', 'person 1')\n",
    "    prompt = prompt.replace('agent_2', 'person 2')\n",
    "    return prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainer\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir=f'rg_output_vicuna',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-3\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "      lora_alpha=16,\n",
    "      lora_dropout=0.1,\n",
    "      r=64,\n",
    "      bias=\"none\",\n",
    "      task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[f'train'],\n",
    "    eval_dataset = dataset[f'test'],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=form_func,\n",
    "    args=trainingArgs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic-responder-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
