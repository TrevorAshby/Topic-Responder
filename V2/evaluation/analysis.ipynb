{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trevor/TR/topic-responder-venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import json\n",
    "import pickle\n",
    "import codecs\n",
    "import networkx as nx\n",
    "from evaluate import load\n",
    "import torch\n",
    "import numpy\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cot_tokenizer = AutoTokenizer.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "cot_model = AutoModelForSeq2SeqLM.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "cot_model.load_state_dict(torch.load('../CoT/topic_extraction/model/topic_er3.pt'))\n",
    "\n",
    "cot_model = cot_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cot(text_in, tok_in, mod_in):\n",
    "    tok_text = tok_in(text_in, return_tensors='pt').to('cuda:0')\n",
    "    gen_text = mod_in.generate(**tok_text, max_length=60)\n",
    "    dec_text = tok_in.decode(gen_text[0], skip_special_tokens=True)\n",
    "    return dec_text\n",
    "\n",
    "def CoT_to_Preference(cot):\n",
    "    # (sports,yes)|(football team,yes)\n",
    "    # \"{\\\"sports\\\":\\\"positive\\\", \\\"football\\\":\\\"positive\\\"}\"\n",
    "    topics = cot.split('|')\n",
    "    top_dict = {}\n",
    "    for top in topics:\n",
    "        top = top.replace('(', '')\n",
    "        top = top.replace(')', '')\n",
    "        the_top, pref = top.split(',')\n",
    "        # print(pref)\n",
    "        if pref == 'yes':\n",
    "            pref = 'positive'\n",
    "        elif pref == 'no':\n",
    "            pref = 'negative'\n",
    "        else:\n",
    "            pref = 'unknown'\n",
    "        top_dict[the_top] = pref\n",
    "    return top_dict\n",
    "\n",
    "def update_graph(top_pref_prof, g):\n",
    "    prev_tpxt = []\n",
    "    for tpxt in top_pref_prof:\n",
    "        # add node if not in graph, else update it\n",
    "        if tpxt not in g.nodes:\n",
    "            g.add_node(tpxt, pref=top_pref_prof[tpxt])\n",
    "        else:\n",
    "            g.nodes[tpxt]['pref'] = top_pref_prof[tpxt]\n",
    "            \n",
    "        # add all links between nodes in chain if not already existing only if more than 1 node\n",
    "        if len(top_pref_prof) > 1 and len(prev_tpxt) >= 1:\n",
    "            for pt in prev_tpxt:\n",
    "                if (pt.split(',')[0], tpxt.split(',')[0]) not in g.edges:\n",
    "                    g.add_edge(pt.split(',')[0], tpxt.split(',')[0])\n",
    "        # prev_tpxt = tpxt\n",
    "        prev_tpxt.append(tpxt)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Extracted Topic File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:08<00:00,  2.57it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.60it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.39it/s]\n",
      "100%|██████████| 23/23 [00:08<00:00,  2.60it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.62it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.55it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.47it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.42it/s]\n",
      "100%|██████████| 21/21 [00:09<00:00,  2.29it/s]\n",
      "100%|██████████| 35/35 [00:13<00:00,  2.53it/s]\n",
      "100%|██████████| 21/21 [00:10<00:00,  2.03it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.59it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.75it/s]\n",
      "100%|██████████| 24/24 [00:09<00:00,  2.47it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.51it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.59it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.64it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.82it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.57it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.70it/s]\n",
      "100%|██████████| 23/23 [00:09<00:00,  2.47it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.44it/s]\n",
      "100%|██████████| 22/22 [00:08<00:00,  2.60it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.52it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.66it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.46it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.72it/s]\n",
      "100%|██████████| 22/22 [00:09<00:00,  2.34it/s]\n",
      "100%|██████████| 23/23 [00:08<00:00,  2.61it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.79it/s]\n",
      "100%|██████████| 22/22 [00:07<00:00,  3.01it/s]\n",
      "100%|██████████| 22/22 [00:07<00:00,  2.76it/s]\n",
      "100%|██████████| 23/23 [00:08<00:00,  2.58it/s]\n",
      "100%|██████████| 24/24 [00:08<00:00,  2.75it/s]\n",
      "100%|██████████| 22/22 [00:08<00:00,  2.55it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.63it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.78it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.66it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.72it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.67it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.66it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.39it/s]\n",
      "100%|██████████| 22/22 [00:07<00:00,  2.77it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.38it/s]\n",
      "100%|██████████| 22/22 [00:07<00:00,  2.75it/s]\n",
      "100%|██████████| 21/21 [00:06<00:00,  3.02it/s]\n",
      "100%|██████████| 21/21 [00:07<00:00,  2.77it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.57it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.36it/s]\n",
      "100%|██████████| 21/21 [00:08<00:00,  2.48it/s]\n"
     ]
    }
   ],
   "source": [
    "tcds = json.loads(open('./topical_chat/Topical-Chat-master/conversations/train.json', 'r').read())\n",
    "\n",
    "\n",
    "save_js = {}\n",
    "\n",
    "for i, t in enumerate(tcds):\n",
    "    if i == 50:\n",
    "        break\n",
    "    #print(t)\n",
    "    #print(tcds[t]['content'])\n",
    "\n",
    "    graph = nx.Graph()\n",
    "    # agent_1 is user?\n",
    "    conv_list = []\n",
    "    utterance = None\n",
    "    ground_truth = None\n",
    "    for j, msg in enumerate(tqdm(tcds[t]['content'])):\n",
    "        is_issue = False\n",
    "        if msg['agent'] == 'agent_1':\n",
    "            utterance = msg['message']\n",
    "            # generate the graph\n",
    "            try:\n",
    "                topic_xtract = generate_cot(utterance, cot_tokenizer, cot_model)\n",
    "                topic_pref_profile = CoT_to_Preference(topic_xtract.strip())\n",
    "                update_graph(topic_pref_profile, graph)\n",
    "                focus_topic = list(topic_pref_profile.keys())[-1]\n",
    "            except:\n",
    "                is_issue = True\n",
    "\n",
    "        else:\n",
    "            ground_truth = msg['message']\n",
    "        \n",
    "        #print(msg)\n",
    "        #print(j)\n",
    "        if is_issue:\n",
    "            utterance = 'Nothing'\n",
    "            ground_truth = 'Nothing'\n",
    "            focus_topic = 'Nothing'\n",
    "            pickled = 'Nothing'\n",
    "            temp = {'utterance':utterance,'ground_truth':ground_truth, 'focus_topic':focus_topic,'graph':pickled}\n",
    "            conv_list.append(temp)\n",
    "\n",
    "            utterance = None\n",
    "            ground_truth = None\n",
    "        if utterance != None and ground_truth != None:\n",
    "            # make graph string\n",
    "            pickled = codecs.encode(pickle.dumps(graph), \"base64\").decode()\n",
    "            temp = {'utterance':utterance,'ground_truth':ground_truth, 'focus_topic':focus_topic,'graph':pickled}\n",
    "            conv_list.append(temp)\n",
    "            utterance = None\n",
    "            ground_truth = None\n",
    "    \n",
    "    save_js[t] = conv_list\n",
    "\n",
    "with open('./eval_ds.json', 'w') as fp:\n",
    "    json.dump(save_js, fp)\n",
    "# generate a file that is [conversation_id, utterance, ground_truth, graph]\n",
    "# pickled = codecs.encode(pickle.dumps(graph), \"base64\").decode()\n",
    "# unpickled = pickle.loads(codecs.decode(pickled.encode(), \"base64\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "ref = ['the','dog','jumped','over','the','moon']\n",
    "hyp = ['the','dog','jumped','over','the','moon']\n",
    "\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([ref], hyp)\n",
    "print(BLEUscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Score\n",
    "https://medium.com/mlearning-ai/text-summarization-84ada711c49c\n",
    "\n",
    "Rouge1 = unigram overlap\n",
    "\n",
    "Rouge2 = bigram overalap\n",
    "\n",
    "RougeL = Longest Common Subsequence (LCS)\n",
    "\n",
    "(RougeW ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.8571428571428571, 'rouge2': 0.4, 'rougeL': 0.8571428571428571, 'rougeLsum': 0.8571428571428571}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "candidates = [\"Summarization is cool\"]\n",
    "\n",
    "references = [[\"Summarization is very cool\"]]\n",
    "results = rouge.compute(predictions=candidates, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UniEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"MingZhong/unieval-dialog\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"MingZhong/unieval-dialog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating naturalness of 1 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating coherence of 1 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating engagingness of 1 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating groundedness of 1 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating understandability of 1 samples !!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation scores are shown below:\n",
      "+-------------------+----------+\n",
      "|     Dimensions    |  Score   |\n",
      "+-------------------+----------+\n",
      "|    naturalness    | 0.510111 |\n",
      "|     coherence     | 0.000146 |\n",
      "|    engagingness   | 0.00046  |\n",
      "|    groundedness   | 0.430141 |\n",
      "| understandability | 0.489637 |\n",
      "|      overall      | 0.286099 |\n",
      "+-------------------+----------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from UniEval.UniEval.utils import convert_to_json\n",
    "from UniEval.UniEval.metric.evaluator import get_evaluator\n",
    "\n",
    "task = 'dialogue'\n",
    "\n",
    "# a list of dialogue histories\n",
    "src_list = ['hi , do you know much about the internet ? \\n i know a lot about different sites and some website design , how about you ? \\n\\n']\n",
    "# a list of additional context that should be included into the generated response\n",
    "context_list = ['']#['the 3 horizontal line menu on apps and websites is called a hamburger button .\\n']\n",
    "# a list of model outputs to be evaluated\n",
    "output_list = ['i like pizza.']# ['i do too . did you know the 3 horizontal line menu on apps and websites is called the hamburger button ?']\n",
    "\n",
    "# Prepare data for pre-trained evaluators\n",
    "data = convert_to_json(output_list=output_list, \n",
    "                       src_list=src_list, context_list=context_list)\n",
    "\n",
    "# Initialize evaluator for a specific task\n",
    "evaluator = get_evaluator(task)\n",
    "# Get multi-dimensional evaluation scores\n",
    "eval_scores = evaluator.evaluate(data, print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "  \n",
    "openai.my_api_key = open('./api_key.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendation(text_in, tok_in, mod_in):\n",
    "    tok_text = tok_in(text_in, return_tensors='pt').to('cuda:0')\n",
    "    gen_text = mod_in.generate(**tok_text, max_new_tokens=32)\n",
    "    dec_text = tok_in.decode(gen_text[0], skip_special_tokens=True)\n",
    "    return dec_text\n",
    "\n",
    "def generate_response(text_in, guideline, tok_in, mod_in):\n",
    "    blend_in_str = text_in + ' [GUIDELINE] ' + guideline\n",
    "    blend_in_ids = tok_in([blend_in_str], max_length=512, return_tensors='pt', truncation=True)\n",
    "    blend_example = mod_in.generate(**blend_in_ids, max_length=60)\n",
    "    response = tok_in.batch_decode(blend_example, skip_special_tokens=True)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trevi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "blen_tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "blen_model = AutoModelForSeq2SeqLM.from_pretrained(\"TrevorAshby/blenderbot-400M-distill\")\n",
    "blen_model = blen_model.cuda()\n",
    "\n",
    "recommender_tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "recommender_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\")\n",
    "recommender_model.load_state_dict(torch.load('../CoT/recommender/model/rec_er.pt'))\n",
    "recommender_model = recommender_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.09s/it]\n",
      "100%|██████████| 10/10 [00:32<00:00,  3.30s/it]\n",
      "100%|██████████| 10/10 [00:32<00:00,  3.24s/it]\n",
      "100%|██████████| 11/11 [00:35<00:00,  3.22s/it]\n",
      "100%|██████████| 10/10 [00:31<00:00,  3.18s/it]\n",
      "100%|██████████| 10/10 [00:31<00:00,  3.13s/it]\n",
      "100%|██████████| 10/10 [00:32<00:00,  3.23s/it]\n",
      "100%|██████████| 10/10 [00:30<00:00,  3.03s/it]\n",
      "100%|██████████| 10/10 [00:32<00:00,  3.30s/it]\n",
      "100%|██████████| 17/17 [00:54<00:00,  3.21s/it]\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "eval_tcds = json.loads(open('./eval_ds.json', 'r').read())\n",
    "\n",
    "save_js = {}\n",
    "\n",
    "for i, t in enumerate(eval_tcds):\n",
    "    if i == 10:\n",
    "        break\n",
    "    conv_list = []\n",
    "    for j, inst in enumerate(tqdm(eval_tcds[t])):\n",
    "        #print(eval_tcds[t])\n",
    "        user_in = inst['utterance']\n",
    "        real_response = inst['ground_truth']\n",
    "        # unpickle\n",
    "        pickled = inst['graph']\n",
    "        focus_topic = inst['focus_topic']\n",
    "        unpickled = pickle.loads(codecs.decode(pickled.encode(), \"base64\"))\n",
    "\n",
    "        xtract_prof = {}\n",
    "        xtract_prof[focus_topic] = unpickled.nodes[focus_topic]['pref']\n",
    "        for x_nodes in unpickled.edges([focus_topic]):\n",
    "            xn = x_nodes[1]\n",
    "            xtract_prof[xn] = unpickled.nodes[xn]['pref']\n",
    "\n",
    "        num_sugg = 3\n",
    "        prompt = f\"Instruction: Generate only {num_sugg} similar topics that could be suggested for new conversation that takes influence from but are not present in the following user profile: {xtract_prof} In the generated answer, generate each of the suggested topics separated by a comma like so: TOPIC1,TOPIC2,TOPIC3,TOPIC4,etc.\\nSuggested Topics:\"\n",
    "        topic_recs = generate_recommendation(prompt, recommender_tokenizer, recommender_model).split(',')\n",
    "\n",
    "        # template guideline generation\n",
    "        if xtract_prof[focus_topic] == 'positive':\n",
    "            tpref = 'The user likes'\n",
    "        elif xtract_prof[focus_topic] == 'negative':\n",
    "            tpref = 'The user dislikes'\n",
    "        else:\n",
    "            tpref = 'It is unclear if the user likes or dislikes'\n",
    "\n",
    "        guideline = f'{tpref} {focus_topic}. Direct the conversation to one of the following 3 topics: {topic_recs}.'\n",
    "\n",
    "        # response generate\n",
    "\n",
    "        blend_in_ids = blen_tokenizer([f'{user_in} [GUIDELINE] {guideline}'], max_length=128, return_tensors='pt', truncation=True).to('cuda:0')\n",
    "        blend_example = blen_model.generate(**blend_in_ids, max_length=60)\n",
    "        blend_response = blen_tokenizer.batch_decode(blend_example, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "        # GENERATE BLEU SCORE\n",
    "        # split all words\n",
    "        our_bleu = nltk.translate.bleu_score.sentence_bleu([blend_response], real_response)\n",
    "        \n",
    "        # GENERATE ROUGE SCORE\n",
    "        #print(blend_response.strip())\n",
    "        #print(real_response)\n",
    "        our_rouge = rouge.compute(predictions=[blend_response], references=[[real_response]])\n",
    "        #print(our_rouge)\n",
    "\n",
    "        # GENERATE CHATGPT SCORE\n",
    "        messages = [ {\"role\": \"system\", \"content\": \"You are a intelligent assistant.\"} ]\n",
    "        message = blend_response\n",
    "        if message: \n",
    "            prompt = f'You job is to rank on a scale of 1-5 how well utterance B responds to utterance A:\n",
    "            A: \"{user_in}\"\n",
    "            B: \"{message}\"'\n",
    "            messages.append( \n",
    "                {\"role\": \"user\", \"content\": prompt}, \n",
    "            ) \n",
    "            chat = openai.ChatCompletion.create( \n",
    "                model=\"gpt-3.5-turbo\", messages=messages \n",
    "            ) \n",
    "        reply = chat.choices[0].message.content \n",
    "        print(f\"ChatGPT: {reply}\") \n",
    "        # messages.append({\"role\": \"assistant\", \"content\": reply}) \n",
    "        \n",
    "        ours = {'generated':blend_response, \n",
    "                'bleu':our_bleu, \n",
    "                'rouge':our_rouge, \n",
    "                'guideline':guideline,\n",
    "                'suggested_topics':topic_recs,\n",
    "                'focus_topic':focus_topic,\n",
    "                'pref_prof':xtract_prof} \n",
    "        temp = {'user_in':user_in, 'ours':ours, 'target_response':real_response}\n",
    "        conv_list.append(temp)\n",
    "        # save [input, our output, bleu, rouge, output baselines, bleu, rouge, GPT-4 ranking]\n",
    "    save_js[t] = conv_list\n",
    "\n",
    "with open('./eval_final.json', 'w') as fp:\n",
    "    json.dump(save_js, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
