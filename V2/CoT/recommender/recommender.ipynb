{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import neo4j_memory.neo4j_helper as neo4j_helper\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoConfig,\\\n",
    "      T5ForConditionalGeneration, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Pass input into topic extraction\n",
    "\n",
    "# download the models\n",
    "cot_tokenizer = AutoTokenizer.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "cot_model = AutoModelForSeq2SeqLM.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "cot_model.load_state_dict(torch.load('../topic_extraction/model/topic_er2.pt'))\n",
    "\n",
    "sent_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "sent_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "config = AutoConfig.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "inst_tokenizer = AutoTokenizer.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "inst_model = AutoModelForSeq2SeqLM.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "\n",
    "# chain of topics\n",
    "def extract_topic_sentiment(text_in):\n",
    "    instruct_input = \"Instruction:What is the topic of conversation?\\n\\nInput:[CONTEXT]{}[ENDOFDIALOGUE][QUESTION]The topic of conversation is\".format(text_in)\n",
    "    tokens_input = inst_tokenizer(instruct_input, max_length=250, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    input_out = inst_model.generate(**tokens_input)\n",
    "    topic = inst_tokenizer.decode(input_out[0], skip_special_tokens=True)\n",
    "\n",
    "    tokens_input = sent_tokenizer(text_in, max_length=250, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    input_out = sent_model(**tokens_input)\n",
    "\n",
    "    scores = softmax(input_out[0][0].detach().numpy())\n",
    "    #print(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = config.id2label[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "        print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "\n",
    "    return topic, config.id2label[ranking[0]]\n",
    "\n",
    "def generate_cot(text_in):\n",
    "    tok_text = cot_tokenizer(text_in, return_tensors='pt')\n",
    "    gen_text = cot_model.generate(**tok_text)\n",
    "    dec_text = cot_tokenizer.decode(gen_text[0], skip_special_tokens=True)\n",
    "    return dec_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trevi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) positive 0.9492\n",
      "2) neutral 0.0482\n",
      "3) negative 0.0026\n",
      "CoT:video games,yes)|(super mario bros,yes)\n",
      ", Topic:Mario, Sent:positive\n"
     ]
    }
   ],
   "source": [
    "in_str = \"I like playing the super mario bros.\"\n",
    "topic, sent = extract_topic_sentiment(in_str)\n",
    "dec_out = generate_cot(in_str)\n",
    "print(f\"CoT:{dec_out}, Topic:{topic}, Sent:{sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you a fan of Google or Microsoft? agent_1\n",
      "Both are excellent technology they are helpful in many ways. For the security purpose both are super. agent_2\n",
      "I'm not  a huge fan of Google, but I use it a lot because I have to. I think they are a monopoly in some sense.  agent_1\n",
      "Google provides online related services and products, which includes online ads, search engine and cloud computing. agent_2\n",
      "Yeah, their services are good. I'm just not a fan of intrusive they can be on our personal lives.  agent_1\n",
      "Google is leading the alphabet subsidiary and will continue to be the Umbrella company for Alphabet internet interest. agent_2\n",
      "Did you know Google had hundreds of live goats to cut the grass in the past? \n",
      " agent_1\n",
      "It is very interesting. Google provide \"Chrome OS\" which is a light weight OS. Google provided a lot of hardware mainly in 2010 to 2015.  agent_2\n",
      "I like Google Chrome. Do you use it as well for your browser?  agent_1\n",
      "Yes.Google is the biggest search engine and Google service figure out top 100 website, including Youtube and Blogger. agent_2\n",
      "By the way, do you like Fish?  agent_1\n",
      "Yes. They form a sister group of tourniquets- they make the sea water clean and remove the dust from it. Fish is the biggest part in the eco-system. agent_2\n",
      "Did you know that a seahorse is the only fish to have a neck?  agent_1\n",
      "Freshwater fish only drink water through the skin via Osmosis, Saltwater fish drink water through the mouth. Dolphins are friendly to human beings. agent_2\n",
      "Interesting, they also have gills. Did you know that jellyfish are immortal?  agent_1\n",
      "Yes. Fish is the important resources of human world wide for the commercial and subsistence fish hunts the fish in the wild fisheries. agent_2\n",
      "What about cats, do you like cats? I'm a dog fan myself.  agent_1\n",
      "The cat is referred as domestic cat and wild cat. They make our world very clean from rats!  agent_2\n",
      "Yeah, cats can be cool, but they sure do spend a lot of their time sleeping.  agent_1\n",
      "Cats hear the sounds too faint or too high frequency human ears can hear.  agent_2\n",
      "I heard that too. Well, it was nice chatting with you. Have a good day.  agent_1\n"
     ]
    }
   ],
   "source": [
    "# load the input\n",
    "with open('../topical_chat/Topical-Chat-master/conversations/train.json', 'r') as jsonfile:\n",
    "    topical_chat_conversations = json.load(jsonfile)\n",
    "    instance = topical_chat_conversations[list(topical_chat_conversations.keys())[0]]['content']\n",
    "    \n",
    "    for x in instance:\n",
    "        print(x['message'], x['agent'])\n",
    "        # print('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Nodes for each topic in CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trevi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Recommender model\n",
    "recommender_tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "recommender_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\")\n",
    "recommender_model.load_state_dict(torch.load('./model/rec_er.pt'))\n",
    "recommender_model.eval()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendation(text_in):\n",
    "    tok_text = recommender_tokenizer(text_in, return_tensors='pt')\n",
    "    gen_text = recommender_model.generate(**tok_text, max_new_tokens=32)\n",
    "    dec_text = recommender_tokenizer.decode(gen_text[0], skip_special_tokens=True)\n",
    "    return dec_text\n",
    "\n",
    "    # Input: CoT, All nodes that are 1 distance from current topic\n",
    "    # Output: New suggested topic CoT\n",
    "def generate_rec2(text_in):\n",
    "    tok_text = recommender_tokenizer(text_in, return_tensors='pt')\n",
    "    print(tok_text)\n",
    "    gen_text = recommender_model(input_ids=tok_text.input_ids, labels=tok_text.input_ids)\n",
    "    #dec_text = recommender_tokenizer.decode(gen_text[0], skip_special_tokens=True)\n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen:  basketball,music appreciation,sports controversies\n"
     ]
    }
   ],
   "source": [
    "text_in = \"Instruction: Generate only 4 similar topics that could be suggested for new conversation that takes influence from but are not present in the following user profile: {\\\"sports\\\":\\\"positive\\\", \\\"football\\\":\\\"positive\\\", \\\"nflteams\\\":\\\"positive\\\"} In the generated answer, generate the suggested topic within brackets [SUGGESTEDTOPIC]\\nAnswer:\"\n",
    "\n",
    "num_sugg = 3\n",
    "#inp = \"{\\\"sports\\\":\\\"positive\\\", \\\"football\\\":\\\"positive\\\", \\\"nflteams\\\":\\\"positive\\\"}\"\n",
    "#inp = \"{\\\"food\\\":\\\"positive\\\", \\\"cheeseburger\\\":\\\"positive\\\", \\\"fry sauce\\\":\\\"positive\\\", \\\"mcdonalds\\\":\\\"positive\\\"}\"\n",
    "#inp = \"{\\\"food\\\":\\\"positive\\\", \\\"cheeseburger\\\":\\\"negative\\\", \\\"chicken nuggets\\\":\\\"positive\\\", \\\"mcdonalds\\\":\\\"positive\\\"}\"\n",
    "#inp = \"{\\\"movies\\\":\\\"positive\\\", \\\"sci-fi\\\":\\\"positive\\\", \\\"star wars\\\":\\\"positive\\\", \\\"darth vader\\\":\\\"positive\\\"}\"\n",
    "#inp = \"{\\\"animals\\\":\\\"positive\\\", \\\"zoo\\\":\\\"positive\\\", \\\"pandas\\\":\\\"positive\\\"}\"\n",
    "#inp = \"{\\\"sports\\\":\\\"positive\\\", \\\"basketball\\\":\\\"positive\\\"}\"\n",
    "inp = \"{\\\"sports\\\":\\\"negative\\\", \\\"basketball\\\":\\\"negative\\\", \\\"music\\\":\\\"positive\\\", \\\"soccer\\\":\\\"negative\\\"}\"\n",
    "#inp = \"{\\\"education\\\":\\\"positive\\\", \\\"universities\\\":\\\"positive\\\", \\\"virginia tech\\\":\\\"positive\\\", \\\"lifu huang\\\":\\\"positive\\\", \\\"computer science\\\":\\\"positive\\\"}\"\n",
    "prompt = f\"Instruction: Generate only {num_sugg} similar topics that could be suggested for new conversation that takes influence from but are not present in the following user profile: {inp} In the generated answer, generate each of the suggested topics separated by a comma like so: TOPIC1,TOPIC2,TOPIC3,TOPIC4,etc.\\nSuggested Topics:\"\n",
    "        \n",
    "instruction = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "print('gen: ', generate_recommendation(prompt))\n",
    "#print(generate_rec2(\"I like things.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'technology': 'positive', 'smartphone features': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "def CoT_to_Preference(cot):\n",
    "    # (sports,yes)|(football team,yes)\n",
    "    # \"{\\\"sports\\\":\\\"positive\\\", \\\"football\\\":\\\"positive\\\"}\"\n",
    "    topics = cot.split('|')\n",
    "    top_dict = {}\n",
    "    for top in topics:\n",
    "        top = top.replace('(', '')\n",
    "        top = top.replace(')', '')\n",
    "        the_top, pref = top.split(',')\n",
    "        #print(pref)\n",
    "        if pref == 'yes':\n",
    "            pref = 'positive'\n",
    "        elif pref == 'no':\n",
    "            pref = 'negative'\n",
    "        else:\n",
    "            pref = 'unknown'\n",
    "        top_dict[the_top] = pref\n",
    "    return top_dict\n",
    "\n",
    "print(CoT_to_Preference('(technology,yes)|(smartphone features,yes)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET RESPONSE: Are you a fan of Google or Microsoft?\n",
      "\n",
      "Both are excellent technology they are helpful in many ways. For the security purpose both are super.|(technology,yes)|(smartphone features,yes)|{'technology': 'positive', 'smartphone features': 'positive'}|smartphone features,smartphone apps,smartwatches\n",
      "TARGET RESPONSE: I'm not  a huge fan of Google, but I use it a lot because I have to. I think they are a monopoly in some sense. \n",
      "\n",
      "Google provides online related services and products, which includes online ads, search engine and cloud computing.|(technology,yes)|(virtual reality,yes)|{'technology': 'positive', 'virtual reality': 'positive'}|virtual reality,ethics of VR,virtual reality experiences\n",
      "TARGET RESPONSE: Yeah, their services are good. I'm just not a fan of intrusive they can be on our personal lives. \n",
      "\n",
      "Google is leading the alphabet subsidiary and will continue to be the Umbrella company for Alphabet internet interest.|(technology,yes)|(internet interest,yes)|{'technology': 'positive', 'internet interest': 'positive'}|internet trends,tech tips,internet security\n",
      "TARGET RESPONSE: Did you know Google had hundreds of live goats to cut the grass in the past? \n",
      "\n",
      "\n",
      "It is very interesting. Google provide \"Chrome OS\" which is a light weight OS. Google provided a lot of hardware mainly in 2010 to 2015. |(technology,yes)|(smartphone,yes)|{'technology': 'positive', 'smartphone': 'positive'}|technology,smartphones,technology trends\n",
      "TARGET RESPONSE: I like Google Chrome. Do you use it as well for your browser? \n",
      "\n",
      "Yes.Google is the biggest search engine and Google service figure out top 100 website, including Youtube and Blogger.|(technology,yes)|(search,yes)|{'technology': 'positive', 'search': 'positive'}|technology,search techniques,technology applications\n",
      "TARGET RESPONSE: By the way, do you like Fish? \n",
      "\n",
      "Yes. They form a sister group of tourniquets- they make the sea water clean and remove the dust from it. Fish is the biggest part in the eco-system.|(nature,yes)|(ecology,yes)|{'nature': 'positive', 'ecology': 'positive'}|eco-friendly practices,sustainable development,eco-friendly products\n",
      "TARGET RESPONSE: Did you know that a seahorse is the only fish to have a neck? \n",
      "\n",
      "Freshwater fish only drink water through the skin via Osmosis, Saltwater fish drink water through the mouth. Dolphins are friendly to human beings.|(animals,yes)|(dolphins,yes)|{'animals': 'positive', 'dolphins': 'positive'}|dolphin care,fisheries,marine conservation\n",
      "TARGET RESPONSE: Interesting, they also have gills. Did you know that jellyfish are immortal? \n",
      "\n",
      "Yes. Fish is the important resources of human world wide for the commercial and subsistence fish hunts the fish in the wild fisheries.|(fishing,yes)|(wildlife,yes)|{'fishing': 'positive', 'wildlife': 'positive'}|wildlife conservation,fishing gear,fishing gear\n",
      "TARGET RESPONSE: What about cats, do you like cats? I'm a dog fan myself. \n",
      "\n",
      "The cat is referred as domestic cat and wild cat. They make our world very clean from rats! |(pets,yes)|(cat,yes)|{'pets': 'positive', 'cat': 'positive'}|cat care,animal welfare,pet adoption\n",
      "TARGET RESPONSE: Yeah, cats can be cool, but they sure do spend a lot of their time sleeping. \n",
      "\n",
      "Cats hear the sounds too faint or too high frequency human ears can hear. |(pets,yes)|(cat sounds,no)|{'pets': 'positive', 'cat sounds': 'negative'}|cat care,cat sounds,pet adoption\n",
      "TARGET RESPONSE: I heard that too. Well, it was nice chatting with you. Have a good day. \n",
      "\n",
      "TARGET RESPONSE: do you like dance?\n",
      "\n",
      "Yes  I do. Did you know Bruce Lee was a cha cha dancer?|(dancing,yes)|(Bruce Lee,yes)|{'dancing': 'positive', 'Bruce Lee': 'positive'}|Bruce Lee,Dancing,music appreciation\n",
      "TARGET RESPONSE: Yes he even won a hardcore cha cha championship in 1958\n",
      "\n",
      "Yeah. Did you know Tupac was a ballet dancer?|(dancing,yes)|(Tupac,unknown)|{'dancing': 'positive', 'Tupac': 'unknown'}|dance styles,rapping styles,music appreciation\n",
      "TARGET RESPONSE: Yes and he even was in the production of the nutcracker\n",
      "\n",
      "Yeah. Ballet dancer go through 4 pairs of shoes a week|(fitness,yes)|(shoes,no)|{'fitness': 'positive', 'shoes': 'negative'}|fitness routines,shopping tips,shopping destinations\n",
      "TARGET RESPONSE: Yes that is a lot of shoes and also a lot of money\n",
      "\n",
      "Yeah true. Did you know babies are really good at dancing?|(dancing,yes)|(babies,yes)|{'dancing': 'positive', 'babies': 'positive'}|dancing,babies,dancing styles\n",
      "TARGET RESPONSE: Yes and they smile more when they hit the beat\n",
      "\n",
      "Yeah they are much smarter than we give them credit for|(technology,yes)|(smartness,yes)|{'technology': 'positive', 'smartness': 'positive'}|technology,smart devices,technology trends\n",
      "TARGET RESPONSE: True Did you know Jackson had a patent on a dancing device?\n",
      "\n",
      "Yes it helped him smooth out his dance moves|(dancing,yes)|(ballet,yes)|{'dancing': 'positive', 'ballet': 'positive'}|ballet techniques,ballet costumes,ballet ensembles\n",
      "TARGET RESPONSE: Nice. Do you like Shakespeare?\n",
      "\n",
      "Yes I do. Do you know that he popularized many phrases|(music,yes)|(music artist,unknown)|{'music': 'positive', 'music artist': 'unknown'}|music appreciation,music history,music festivals\n",
      "TARGET RESPONSE: Yes like good riddance, in my heart of hearts and such\n",
      "\n",
      "Yes and then he also invented names like Jessica, Olivia and Miranda|(TV shows,yes)|(TV show characters,yes)|{'TV shows': 'positive', 'TV show characters': 'positive'}|TV series,movie characters,TV personalities\n",
      "TARGET RESPONSE: Yes. And for his works you have to use old english for it to make sense\n",
      "\n",
      "Yes otherwise the rhymes and puns do not seem to work out|(fitness,no)|{'fitness': 'negative'}|fitness goals,fitness challenges,nutrition tips\n",
      "TARGET RESPONSE: Yes. He lived at the same time as Pocahontas too\n",
      "\n",
      "I wonder if they met how that would go from there|(movie,yes)|(movie plot,unknown)|{'movie': 'positive', 'movie plot': 'unknown'}|movie reviews,movie adaptations,movie quotes\n",
      "TARGET RESPONSE: Yeah interesting point. Nice chat\n",
      "\n",
      "TARGET RESPONSE: Hey what's up do use Google very often?I really love the company and was surprised to hear that it was founded back in 1998.\n",
      "\n",
      "i think everyone must use it daily! its become ingrained in every day life|(technology,yes)|(smartphone usage,yes)|{'technology': 'positive', 'smartphone usage': 'positive'}|smartphone apps,smartphone features,smartphone apps\n",
      "TARGET RESPONSE: Agreed. The Google headquarters in Mountain View California is nicknamed the Google Plex.\n",
      "\n",
      "thats funny. The current CEO is Sundar Pichai, i didnt know Larry Page was replaced|(work,yes)|(corporate,no)|(Larry Page,|{'work': 'positive', 'corporate': 'negative', 'Larry Page': 'unknown'}|ad agencies,corporate culture,personal development\n",
      "TARGET RESPONSE: Oh yeah I didn't know that either. I also want to go to google Plex to see the goats who mow their lawn by eating it.\n",
      "\n",
      "say what now?? they have that??|(technology,yes)|(smart watch,yes)|{'technology': 'positive', 'smart watch': 'positive'}|technology,smart watches,tech trends\n",
      "TARGET RESPONSE: Yeah apparently lol! They do that instead of hiring people to mow!\n",
      "\n",
      "thats both funny and i guess imaginative. leave it to a huge tech company to employ actual goats!|(technology,yes)|(taxes,no)|{'technology': 'positive', 'taxes': 'negative'}|tax reform,environmental issues\n",
      "TARGET RESPONSE: Yeah exactly I am sure they are cheaper. One thing I bet they couldn't exploit is fish. I think fish are so cool there is actually a breed of jellyfish that is immortal.\n",
      "\n",
      "i had rememered hearing about that before. Immortatlity is wasted on a jellyfish haha. did you know a seahorse is the only fish that has an actual neck?|(food,yes)|(seahorse,yes)|{'food': 'positive', 'seahorse': 'positive'}|seafood recipes,fish species,sea life\n",
      "TARGET RESPONSE: That is so funny I guess I never considered a seahorse a fish. The black swallower fish sounds a lot like a snake because it can eat pray that is so large.\n",
      "\n",
      "i guess they live up to their name then!|(movie,yes)|(movie characters,yes)|{'movie': 'positive', 'movie characters': 'positive'}|movie reviews,movie trailers,movie quotes\n",
      "TARGET RESPONSE: It seems they do. I also didn't know that there was a difference with how freshwater and saltwater fish drink.\n",
      "\n",
      "thats crazy. i wonder why fresh water ones only use osmosis? |(water,yes)|(fresh water,unknown)|{'water': 'positive', 'fresh water': 'unknown'}|water quality,water quality monitoring,water quality\n",
      "TARGET RESPONSE: Yeah and saltwater fish are lucky because they can do that and drink through their mouth's.\n",
      "\n",
      "seems like fresh water fish got the short end of the stick with that one. Have you ever been to a cat cafe?|(cat cafe,yes)|{'cat cafe': 'positive'}|cat cafes,cat care,cat adoption\n",
      "TARGET RESPONSE: I have never been to a cat cafe no, what about you? Seems like they are popular in Japan and Taiwan.\n",
      "\n",
      "no but I would love to! paying hourly to hang out with adorable cats? im in!|(pets,yes)|(cats,yes)|{'pets': 'positive', 'cats': 'positive'}|cat care,animal welfare,pet adoption\n",
      "TARGET RESPONSE: Yeah that would be a lot of fun. I didn't realize that cats sleep so much. Must be nice.\n",
      "\n",
      "i guess thats where the phrase \"cat nap\" comes from|(cat,no)|(cat nap,no)|{'cat': 'negative', 'cat nap': 'negative'}|cat care,cat behavior,cat adoption\n",
      "TARGET RESPONSE: Oh yeah I guess so ha ha. There's even a town in Alaska that has a mayor cat.\n",
      "\n",
      "TARGET RESPONSE: Hi!  do you like to dance?\n",
      "\n",
      "I love to dance a lot. How about you?|(dancing,yes)|{'dancing': 'positive'}|dancing,dance moves,dance routines\n",
      "TARGET RESPONSE: I am really bad, but it is a good time.\n",
      "\n",
      "Dancing is a lot of fun. Did you know that Bruce Lee was a great dancer?|(dancing,yes)|(Bruce Lee,yes)|{'dancing': 'positive', 'Bruce Lee': 'positive'}|Bruce Lee,Dancing,music appreciation\n",
      "TARGET RESPONSE: I heard that, winning Cha Cha championships and everything!\n",
      "\n",
      "Yes that is amazing. He won the Hong Kong cha-cha championship back in 1958 in fact.|(sports,yes)|(basketball,yes)|{'sports': 'positive', 'basketball': 'positive'}|basketball,basketball equipment,basketball history\n",
      "TARGET RESPONSE: I always just thought of him as a martial arts legend.  Now he is a dance legend of sorts too!\n",
      "\n",
      "Yeah!! That is correct. He was a fantastic martial artist. Did you know that Tupac danced ballet in high school?|(sports,yes)|(ballet,unknown)|{'sports': 'positive', 'ballet': 'unknown'}|sports,ballet techniques,sports teams\n",
      "TARGET RESPONSE: Yeah!  He was the mouse king in the Nutcracker.  Thats pretty cool, I would definitely never have guessed that about him.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32ma:\\VirginiaTech\\NLPLAB\\Topic-Responder\\V2\\CoT\\recommender\\recommender.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/VirginiaTech/NLPLAB/Topic-Responder/V2/CoT/recommender/recommender.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m cot_out \u001b[39m=\u001b[39m generate_cot(x[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/VirginiaTech/NLPLAB/Topic-Responder/V2/CoT/recommender/recommender.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m cot_out \u001b[39m=\u001b[39m cot_out\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/a%3A/VirginiaTech/NLPLAB/Topic-Responder/V2/CoT/recommender/recommender.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m pref \u001b[39m=\u001b[39m CoT_to_Preference(cot_out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/VirginiaTech/NLPLAB/Topic-Responder/V2/CoT/recommender/recommender.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m num_sugg \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/VirginiaTech/NLPLAB/Topic-Responder/V2/CoT/recommender/recommender.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m inp \u001b[39m=\u001b[39m pref\n",
      "\u001b[1;32ma:\\VirginiaTech\\NLPLAB\\Topic-Responder\\V2\\CoT\\recommender\\recommender.ipynb Cell 10\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/a%3A/VirginiaTech/NLPLAB/Topic-Responder/V2/CoT/recommender/recommender.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m top \u001b[39m=\u001b[39m top\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m(\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/a%3A/VirginiaTech/NLPLAB/Topic-Responder/V2/CoT/recommender/recommender.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m top \u001b[39m=\u001b[39m top\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/a%3A/VirginiaTech/NLPLAB/Topic-Responder/V2/CoT/recommender/recommender.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m the_top, pref \u001b[39m=\u001b[39m top\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/VirginiaTech/NLPLAB/Topic-Responder/V2/CoT/recommender/recommender.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#print(pref)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/a%3A/VirginiaTech/NLPLAB/Topic-Responder/V2/CoT/recommender/recommender.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mif\u001b[39;00m pref \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39myes\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# Validate output / shifting (using Amazon dataset I found)\n",
    "output_file = open('./out_log_ext.txt', 'w')\n",
    "with open('../topical_chat/Topical-Chat-master/conversations/train.json', 'r') as jsonfile:\n",
    "    topical_chat_conversations = json.load(jsonfile)\n",
    "    \n",
    "    for idx in range(len(topical_chat_conversations.keys())):\n",
    "        if idx == 10:\n",
    "            break\n",
    "\n",
    "        instance = topical_chat_conversations[list(topical_chat_conversations.keys())[idx]]['content']\n",
    "        for x in instance:\n",
    "            #print(x['message'], x['agent'])\n",
    "            if x['agent'] == 'agent_2':\n",
    "                # pass input into model\n",
    "                cot_out = generate_cot(x['message'])\n",
    "                cot_out = cot_out.strip()\n",
    "                pref = CoT_to_Preference(cot_out)\n",
    "                \n",
    "                num_sugg = 3\n",
    "                inp = pref\n",
    "                prompt = f\"Instruction: Generate only {num_sugg} similar topics that could be suggested for new conversation that takes influence from but are not present in the following user profile: {inp} In the generated answer, generate each of the suggested topics separated by a comma like so: TOPIC1,TOPIC2,TOPIC3,TOPIC4,etc.\\nSuggested Topics:\"\n",
    "                sugg_topics = generate_recommendation(prompt)\n",
    "                output_file.write(f\"{x['message']}|{cot_out}|{pref}|{sugg_topics}\\n\\n\")\n",
    "                print(f\"{x['message']}|{cot_out}|{pref}|{sugg_topics}\")\n",
    "            else:\n",
    "                output_file.write(f\"TARGET RESPONSE: {x['message']}\\n\")\n",
    "                print(f\"TARGET RESPONSE: {x['message']}\\n\")\n",
    "    output_file.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
