{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trevi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import neo4j_memory.neo4j_helper as neo4j_helper\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoConfig,\\\n",
    "      T5ForConditionalGeneration, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Pass input into topic extraction\n",
    "\n",
    "# download the models\n",
    "cot_tokenizer = AutoTokenizer.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "cot_model = AutoModelForSeq2SeqLM.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "cot_model.load_state_dict(torch.load('../topic_extraction/model/topic_er2.pt'))\n",
    "\n",
    "sent_tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "sent_model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "config = AutoConfig.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "inst_tokenizer = AutoTokenizer.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "inst_model = AutoModelForSeq2SeqLM.from_pretrained(\"prakharz/DIAL-BART0\")\n",
    "\n",
    "# chain of topics\n",
    "def extract_topic_sentiment(text_in):\n",
    "    instruct_input = \"Instruction:What is the topic of conversation?\\n\\nInput:[CONTEXT]{}[ENDOFDIALOGUE][QUESTION]The topic of conversation is\".format(text_in)\n",
    "    tokens_input = inst_tokenizer(instruct_input, max_length=250, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    input_out = inst_model.generate(**tokens_input)\n",
    "    topic = inst_tokenizer.decode(input_out[0], skip_special_tokens=True)\n",
    "\n",
    "    tokens_input = sent_tokenizer(text_in, max_length=250, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    input_out = sent_model(**tokens_input)\n",
    "\n",
    "    scores = softmax(input_out[0][0].detach().numpy())\n",
    "    #print(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = config.id2label[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "        print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "\n",
    "    return topic, config.id2label[ranking[0]]\n",
    "\n",
    "def generate_cot(text_in):\n",
    "    tok_text = cot_tokenizer(text_in, return_tensors='pt')\n",
    "    gen_text = cot_model.generate(**tok_text)\n",
    "    dec_text = cot_tokenizer.decode(gen_text[0], skip_special_tokens=True)\n",
    "    return dec_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trevi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) positive 0.9802\n",
      "2) neutral 0.0167\n",
      "3) negative 0.0032\n",
      "CoT:(sports,yes)|(football team,yes)\n",
      ", Topic:Playing football, Sent:positive\n"
     ]
    }
   ],
   "source": [
    "in_str = \"My favorite football team is the Kansas City Chiefs.\"\n",
    "topic, sent = extract_topic_sentiment(in_str)\n",
    "dec_out = generate_cot(in_str)\n",
    "print(f\"CoT:{dec_out}, Topic:{topic}, Sent:{sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input\n",
    "with open('./topical_chat/Topical-Chat-master/conversations/train.json', 'r') as jsonfile:\n",
    "    topical_chat_conversations = json.load(jsonfile)\n",
    "    instance = topical_chat_conversations[list(topical_chat_conversations.keys())[0]]['content']\n",
    "    \n",
    "    for x in instance:\n",
    "        print(x['message'], x['agent'])\n",
    "        # print('--')\n",
    "\n",
    "# pass input into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Nodes for each topic in CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Trevi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Recommender model\n",
    "recommender_tokenizer = AutoTokenizer.from_pretrained(\"t5-large\")\n",
    "recommender_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-large\")\n",
    "recommender_model.load_state_dict(torch.load('./model/rec_er.pt'))\n",
    "recommender_model.eval()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendation(text_in):\n",
    "    tok_text = recommender_tokenizer(text_in, return_tensors='pt')\n",
    "    gen_text = recommender_model.generate(**tok_text, max_new_tokens=32)\n",
    "    dec_text = recommender_tokenizer.decode(gen_text[0], skip_special_tokens=True)\n",
    "    return dec_text\n",
    "\n",
    "    # Input: CoT, All nodes that are 1 distance from current topic\n",
    "    # Output: New suggested topic CoT\n",
    "def generate_rec2(text_in):\n",
    "    tok_text = recommender_tokenizer(text_in, return_tensors='pt')\n",
    "    print(tok_text)\n",
    "    gen_text = recommender_model(input_ids=tok_text.input_ids, labels=tok_text.input_ids)\n",
    "    #dec_text = recommender_tokenizer.decode(gen_text[0], skip_special_tokens=True)\n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen:  science fiction,fantasy films,sci-fi literature,star wars characters\n"
     ]
    }
   ],
   "source": [
    "text_in = \"Instruction: Generate only 4 similar topics that could be suggested for new conversation that takes influence from but are not present in the following user profile: {\\\"sports\\\":\\\"positive\\\", \\\"football\\\":\\\"positive\\\", \\\"nflteams\\\":\\\"positive\\\"} In the generated answer, generate the suggested topic within brackets [SUGGESTEDTOPIC]\\nAnswer:\"\n",
    "\n",
    "num_sugg = 4\n",
    "#inp = \"{\\\"sports\\\":\\\"positive\\\", \\\"football\\\":\\\"positive\\\", \\\"nflteams\\\":\\\"positive\\\"}\"\n",
    "#inp = \"{\\\"food\\\":\\\"positive\\\", \\\"cheeseburger\\\":\\\"positive\\\", \\\"fry sauce\\\":\\\"positive\\\", \\\"mcdonalds\\\":\\\"positive\\\"}\"\n",
    "#inp = \"{\\\"food\\\":\\\"positive\\\", \\\"cheeseburger\\\":\\\"negative\\\", \\\"chicken nuggets\\\":\\\"positive\\\", \\\"mcdonalds\\\":\\\"positive\\\"}\"\n",
    "inp = \"{\\\"movies\\\":\\\"positive\\\", \\\"sci-fi\\\":\\\"positive\\\", \\\"star wars\\\":\\\"positive\\\", \\\"darth vader\\\":\\\"positive\\\"}\"\n",
    "#inp = \"{\\\"animals\\\":\\\"positive\\\", \\\"zoo\\\":\\\"positive\\\", \\\"pandas\\\":\\\"positive\\\"}\"\n",
    "#inp = \"{\\\"sports\\\":\\\"positive\\\", \\\"basketball\\\":\\\"positive\\\"}\"\n",
    "#inp = \"{\\\"sports\\\":\\\"negative\\\", \\\"basketball\\\":\\\"negative\\\", \\\"music\\\":\\\"positive\\\", \\\"country\\\":\\\"positive\\\", \\\"soccer\\\":\\\"negative\\\", \\\"baseball\\\":\\\"negative\\\"}\"\n",
    "#inp = \"{\\\"education\\\":\\\"positive\\\", \\\"universities\\\":\\\"positive\\\", \\\"virginia tech\\\":\\\"positive\\\", \\\"lifu huang\\\":\\\"positive\\\", \\\"computer science\\\":\\\"positive\\\"}\"\n",
    "prompt = f\"Instruction: Generate only {num_sugg} similar topics that could be suggested for new conversation that takes influence from but are not present in the following user profile: {inp} In the generated answer, generate each of the suggested topics separated by a comma like so: TOPIC1,TOPIC2,TOPIC3,TOPIC4,etc.\\nSuggested Topics:\"\n",
    "        \n",
    "instruction = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "print('gen: ', generate_recommendation(prompt))\n",
    "#print(generate_rec2(\"I like things.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate output / shifting (using Amazon dataset I found)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
